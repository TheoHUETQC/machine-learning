{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import mlp # Import the mlp module\n",
        "import layer # Import the layer module\n",
        "from mlp import MLP # Still import the MLP class if needed for direct use\n",
        "from utils import generate_data, plot_classification, accuracy, mlp_networkx_view\n",
        "\n",
        "import importlib\n",
        "\n",
        "# Correcting the relu function in layer.py directly\n",
        "with open('/content/layer.py', 'r') as f:\n",
        "    lines = f.readlines()\n",
        "\n",
        "# Assuming relu function is around line 25 in layer.py based on previous tracebacks\n",
        "# This is a robust way to modify the function if its exact line number might shift slightly\n",
        "relu_start_index = -1\n",
        "relu_end_index = -1\n",
        "for i, line in enumerate(lines):\n",
        "    if \"def relu(seld, x)\" in line:\n",
        "        relu_start_index = i\n",
        "    if relu_start_index != -1 and \"return\" in line:\n",
        "        # Assuming the return statement is the end of the relu function\n",
        "        relu_end_index = i\n",
        "        break\n",
        "\n",
        "if relu_start_index != -1 and relu_end_index != -1:\n",
        "    # Replace the relu function implementation\n",
        "    lines[relu_start_index + 1] = \"        return np.maximum(0, x)\\n\"\n",
        "    # Remove any extra lines if the old implementation was longer\n",
        "    for i in range(relu_end_index + 1, relu_start_index + 2, -1): # Adjust range if needed\n",
        "        if i < len(lines):\n",
        "            # Check if line is part of the old relu implementation or next function\n",
        "            # This is a bit tricky, may need manual adjustment if the structure is complex\n",
        "            # For now, just assume the next line after return is fine or empty\n",
        "            pass\n",
        "\n",
        "with open('/content/layer.py', 'w') as f:\n",
        "    f.writelines(lines)\n",
        "\n",
        "importlib.reload(mlp)\n",
        "importlib.reload(layer)\n",
        "\n",
        "# The rest of the cell content, potentially related to parameters and MLP training/prediction,\n",
        "# would follow here if it were part of the original cell.\n",
        "# For this fix, we only need to ensure the layer.py modification and reload.\n",
        "# The original content of this cell is no longer needed after these imports and fixes are applied.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PolKUy3zm5ix",
        "outputId": "a36a51f6-702a-4743-d49c-cb8ce57f9988"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<module 'layer' from '/content/layer.py'>"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# load data\n",
        "df = pd.read_csv('https://docs.google.com/spreadsheets/d/e/2PACX-1vTRfoQAojPwNsw33zUSpBg32DuK2z0WvLPptGOncIXa38tN3RWk9puvelcmsIIhFb_XF12RD7PdqCwf/pub?output=csv')"
      ],
      "metadata": {
        "id": "yVtOjCqjcNiD"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "no_Cholesterol = df['Cholesterol'] == 0\n",
        "no_RestingBP = df['RestingBP'] == 0\n",
        "\n",
        "\n",
        "#class_df_clean excludes rows that have no values == 0 in the above columns\n",
        "class_df_clean = df[~(no_Cholesterol |\n",
        "                                     no_RestingBP )]\n",
        "class_df_clean.describe()\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.compose import make_column_selector, make_column_transformer\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "## Define X and y\n",
        "target = 'HeartDisease'\n",
        "\n",
        "X = class_df_clean.drop(columns=target).copy()\n",
        "y = class_df_clean[target].copy()\n",
        "\n",
        "# Perfoming a train-test-split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42)\n",
        "\n",
        "# Defining a list of features for both the categorical and the numerical columns\n",
        "\n",
        "cat_feature = make_column_selector(dtype_include='object')   # Creating a categorical data selector\n",
        "num_feature = make_column_selector(dtype_include='number')   # Creating a numeric data selector\n",
        "\n",
        "# Instantiating the Transformers\n",
        "\n",
        "impute_cat = SimpleImputer(strategy='most_frequent') #better to keep the ffil but for simplicity reasons I will keep this one\n",
        "impute_num = SimpleImputer(strategy='median') # we can be more sophisticated and make a mean imputation for widht like we did earlier, but I'll keep it simple for now\n",
        "ohe_encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')   # It is a numinal column with no order to it => So I used the One-hot encoding\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Making pipelines for each category\n",
        "\n",
        "pip_cat = make_pipeline(impute_cat, ohe_encoder)\n",
        "pip_num = make_pipeline(impute_num, scaler)\n",
        "\n",
        "# Defining a tuple for each pathway\n",
        "\n",
        "categorical_tup = ('Categorical', pip_cat, cat_feature)\n",
        "numerical_tup = ('Numerical', pip_num, num_feature)\n",
        "\n",
        "# Instantiating the ColumnTransformer\n",
        "\n",
        "col_transformer = ColumnTransformer([numerical_tup, categorical_tup], verbose_feature_names_out=False)\n",
        "col_transformer\n",
        "\n",
        "X_train_processed = col_transformer.fit_transform(X_train)\n",
        "X_test_processed = col_transformer.transform(X_test)"
      ],
      "metadata": {
        "id": "fnKVaG_ncX4v"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# constante de numérisation :\n",
        "Nt = 100 #nombre de prediction\n",
        "\n",
        "# constante du probleme\n",
        "input_size = X_train_processed.shape[1]\n",
        "hide_layer_size1, activation_fonction1 = 8, 'relu'\n",
        "hide_layer_size2, activation_fonction2 = 8, 'relu'\n",
        "hide_layer_size3, activation_fonction3 = 4, 'sigmoid'\n",
        "output_size, activation_fonction4 = 1, 'sigmoid' # 1 car on veut savoir au dessus ou en dessous de g\n",
        "\n",
        "# creer notre reseau de neuronne simple (MLP)\n",
        "mlp = MLP([input_size, hide_layer_size1,hide_layer_size2, hide_layer_size3 , output_size],[activation_fonction1,activation_fonction2,activation_fonction3,activation_fonction4])\n",
        "\n",
        "# entraine notre mlp\n",
        "# Convert y_train to a numpy array and reshape to a column vector\n",
        "mlp.train(X_train_processed, y_train.values.reshape(-1, 1), Nt)\n",
        "\n",
        "# Prédictions finales\n",
        "# Corrected loop to use range() and predict on X_test_processed\n",
        "z_pred = np.array([mlp.forward(X_test_processed[i]) for i in range(len(X_test_processed))]) # donne des valeurs entre 0 et 1\n",
        "z_pred_int =  np.array([mlp.predict_label(X_test_processed[i]) for i in range(len(X_test_processed))]) # donne soit 1 soit 0\n",
        "\n",
        "# precision de notre MLP\n",
        "print(f\"Précision : {accuracy(y_test.values, z_pred_int) * 100:.2f}%\")\n",
        "\n",
        "mlp_networkx_view(mlp)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "id": "fStEvo1hhLr6",
        "outputId": "a987bf7c-1f64-451b-8058-47fdfd1a0beb"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-327718065.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# entraine notre mlp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# Convert y_train to a numpy array and reshape to a column vector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mmlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_processed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# Prédictions finales\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/mlp.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, inputs, z_true, epochs)\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m                 \u001b[0;31m# prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0mz_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m                 \u001b[0;31m# regarde la difference de sortie\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m                 \u001b[0md_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mz_pred\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mz_true\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/mlp.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, output)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;31m#on parcours les couches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# on avance a la couche d apres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/layer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_out\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;31m# on revient en arrière pour modifier les poids et biais (d_out dérivée de la perte par rapport à la sortie de cette couche)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation_fonction\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'sigmoid'\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m           \u001b[0mdelta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md_out\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid_deriv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# élément par élément\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/layer.py\u001b[0m in \u001b[0;36mrelu\u001b[0;34m(seld, x)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseld\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaximum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrelu_deriv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
          ]
        }
      ]
    }
  ]
}